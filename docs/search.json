[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/foreclosure-long/index.html",
    "href": "posts/foreclosure-long/index.html",
    "title": "Evaluating Predictive Model Accuracy in Foreclosure Cases",
    "section": "",
    "text": "ExplanationCode & Analysis\n\n\n\nIn this tab:\n\nIntroduction\nDataset Description\nMethodology\nCurrent Model Approach\nRandom Forest Model\nRecommendations\n\n\n\nIntroduction\nAccurate prediction of foreclosure timelines is critical for optimizing the valuation of distressed mortgage assets. Foreclosure dates significantly impact the estimation of asset recovery values within a mortgage portfolio, which in turn serves as a baseline for portfolio valuation and the structuring of settlements or other termination strategies. The ability to forecast these timelines with precision allows to better manage risk, enhance asset recovery, and make informed decisions regarding portfolio performance. This report examines the current predictive model for foreclosure times and introduces an enhanced approach aimed at improving the accuracy and reliability of these predictions to better support strategic financial decisions.\n\n\nDataset Description\nThe dataset used in this project contains detailed information on mortgage foreclosure cases in Peru, including various legal proceedings associated with each case. This data is crucial for understanding the factors that influence the time it takes to resolve these cases. Key columns in the dataset include:\n\nPortfolio Name: Identifies the portfolio to which the legal case belongs.\nLegal File Name: A unique identifier for each legal case.\nDate: The date of each legal proceeding.\nLegal Stage: The stage of the legal process\nLegal Proceeding: The specific type of legal action taken within the case.\nForeclosure Initial Date: The date when the foreclosure process began.\nResolution Date: The date when the case was resolved.\nTime to Solve: The duration between the start and resolution of the case.\nGeographical Information: Including Zone, Region, and Zone Group, which provide context on where the case is being handled.\nCase Characteristics: Such as whether an appeal (cassation) was involved (Tiene Casacion) and the central stage of the legal process (Etapa Central).\n\n\n\nMethodology for Evaluating Regression Models\nTo ensure our predictive model is accurate and reliable, we use several key metrics to evaluate its performance:\n·       Mean Absolute Error (MAE): This metric shows the average difference between the predicted and actual resolution times. In simpler terms, it tells us, on average, how many months the model’s predictions are off. The lower the MAE, the more accurate the model.\n·       Root Mean Squared Error (RMSE): RMSE is similar to MAE but gives more weight to larger errors. It provides a clearer picture of how often the model makes bigger mistakes in predicting resolution times. Again, lower values indicate a better-performing model.\n·       R-squared (R²): R² explains how well the model’s predictions match the actual outcomes. It’s expressed as a percentage, where a higher value means the model does a good job of capturing the key factors that determine resolution times. For example, an R² of 0.94 means that the model explains 94% of the variations in resolution times, leaving only 6% unexplained.\n\n\nApproach for Current Model\nThe current model predicts the resolution time for foreclosure cases by using historical averages. Specifically, it looks at past cases and calculates the average time it took to resolve cases based on the type of legal proceeding involved. For each new case, the model predicts the resolution time by matching it to similar cases from the past and applying the average time for those cases. The process involves:\n\nData Collection & Cleaning: We gathered and cleaned past foreclosure data to ensure reliability.\nGrouping by Legal Proceeding: Cases were categorized by legal proceeding type, and average resolution times were calculated for each group.\nPrediction: The model predicts resolution time based on the type of proceeding, using historical averages. If no match is found, an overall average is used. This straightforward approach assumes that past performance predicts future outcomes.\n\n\nEvaluation of the Current Model’s Performance\nTo evaluate the effectiveness of our current model, we tested it against foreclosure cases that have already been resolved. This allows us to compare the model’s predictions with the actual resolution times and measure its accuracy. The key metrics we used are:\n\nMean Absolute Error (MAE): The model’s average prediction was off by 16.68 months. This means that, on average, the model’s predictions are almost 17 months away from the actual resolution time.\nRoot Mean Squared Error (RMSE): This metric, which emphasizes larger errors, shows a typical prediction error of 22.06 months. The higher error indicates that the model struggles with cases that deviate significantly from the average.\nR-squared (R²): The model explains only 39% of the variance in resolution times. This low percentage suggests that the model does not capture many of the factors influencing how long a case will take.\n\nWhile the model provides some predictive capability, its accuracy and reliability are inadequate. The significant errors and low variance explanation suggest the need for a more sophisticated approach.\n\n\nAnalysis of the Actual vs. Predicted Graph for the Current Model\nThe Actual vs. Predicted graph compares the model’s predictions with the actual resolution times for foreclosure cases. In an ideal scenario, all data points would lie along the diagonal line (red dashed line), indicating that the predicted values perfectly match the actual values.\n\n\n\nActual vs. Predicted Resolution Times for Current Model\n\n\nHowever, in the graph for the current model:\n\nWide Dispersion: Many points are scattered far from the diagonal line, indicating large prediction errors. This spread reflects the model’s inaccuracies, particularly in cases where the actual resolution time deviates from the average.\nHorizontal and Vertical Lines: These occur because the model uses average times for specific legal proceedings, leading to identical predictions for different cases. This approach doesn’t account for the variability within those cases, resulting in clusters of predictions that don’t match the actual outcomes.\n\nFor example, in a specific case where the actual resolution time was 30 months, the model might have predicted only 15 months based on the average for that type of proceeding. This discrepancy highlights the model’s tendency to underestimate or overestimate the resolution time, depending on how closely a case aligns with the historical average.\n\n\nLimitations of Using Averages for Predicting Time to Solve\nThe graphs in this section displays the distribution of resolution times for the 20 most common legal proceedings. Each bar or line in the graph represents how resolution times are spread out for these specific types of proceedings.\n\n\n\nDistribution of Resolution Times for Top 20 Legal Proceedings\n\n\n\nWide Variance: For many legal proceedings, there is a broad range of resolution times. When the distribution of these times is far from the mean (average), using that average as a prediction leads to significant errors. This is because the average fails to capture the true spread of the data, especially in cases with outliers or skewed distributions.\nSkewed Distributions: In some cases, the distribution is heavily skewed, meaning that a few cases take much longer or shorter than most others. The average gets pulled in one direction, making it a poor predictor for the majority of cases.\n\n\n\n\nFeature Importance for Random Forest Model\n\n\n\nNon-Normal Distributions: Many proceedings have skewed or uneven distributions, meaning that most cases are not near the average time. Using a simple average as a predictor fails to account for these nuances, leading to significant errors.\n\nIn essence, relying on averages oversimplifies the prediction process and fails to account for the variability in actual case outcomes. This can lead to substantial inaccuracies, especially when the distribution is not centered around the mean\n\n\n\nRandom Forest Predictive Model\nTo enhance the accuracy of our foreclosure time predictions, we developed a Random Forest model. This advanced machine learning technique constructs multiple decision trees, each one analyzing different aspects of the data. By averaging the predictions from these trees, the model effectively manages the complexities of the dataset and minimizes the risk of overfitting.\nOur model leverages a wide range of features, such as the sequence of legal proceedings, time intervals between actions, and specific attributes related to the courts handling the cases. By considering these diverse factors, the Random Forest model delivers predictions that are significantly more accurate and reliable\n\nModel Development\n1.     Data Preprocessing: Cleaned dataset, removed irrelevant columns, handled missing values, and filtered for foreclosure cases. This ensures we’re working with accurate, relevant data.\n2.     Feature Engineering: We created features such as Months_Since_Start, Proceeding_Count, Months_Since_Last_Proceeding, Average_Months_Between_Proceedings, Distinct_Proceeding_Types, and Repeated_Proceeding_Count. These features capture key temporal, procedural, and case-specific aspects, improving prediction accuracy.\n3.     Encoding and Scaling: Applied Target Encoding for high-cardinality variables, Label Encoding for other categorical features, and StandardScaler for numerical features. This prepares the data for optimal model performance.\n4.     Model Training: Split the dataset, then trained a Random Forest with 300 trees. This approach leverages multiple decision trees to make robust predictions about foreclosure resolution times.\n\n\nResults\nThe Random Forest model’s performance in predicting foreclosure resolution times is summarized by the following key metrics:\n\nMean Absolute Error (MAE): 4.89 months\nRoot Mean Squared Error (RMSE): 6.99 months\nR-squared (R²): 0.94\n\nThese metrics indicate a high level of accuracy, with the model explaining 94% of the variance in the resolution times, and relatively low prediction errors.\n\n\n\nRandom Forest Model Predictions vs Actual Resolution Times\n\n\n\n\nFeature Importance\nThe bar chart ranks the importance of various features used by the model to make predictions. Added Features like Months_Since_Start, and Proceeding_Count emerge as the most significant factors influencing the resolution time.\nUnderstanding which features most strongly affect the model’s predictions can provide valuable insights. For instance, the high importance of Months_Since_Start and Proceeding_Count highlights the relevance of the duration and frequency of legal actions in determining how long a foreclosure case will take to resolve.\n\n\n\nFeature Importance for Random Forest Model\n\n\nThis visualization provides a clear ranking of the features that have the most significant impact on the model’s predictions. It offers valuable insights into which factors are most influential in determining foreclosure resolution times.\n\n\n\nRecommendations\n\nExpand Features: Expand Features: Enhance the model’s accuracy by incorporating additional features, such as Loan-to-Value (LTV) ratios to help predict whether a foreclosure will be resolved by a third party or the creditor, along with economic indicators or more detailed geographic data.\nExplore Advanced Techniques: Consider testing more advanced machine learning techniques in the future for further improvements\nIntegrate with Financial Tools: Integrate the model’s predictions into existing financial systems to support seamless decision-making and help structure DPO (Discounted Payoff) agreements more effectively.\n\n\n\n\n\nIn this section:\n\nData Loading\nFeature Engineering\nEncoding and Preparation\nModel Training\nVisualization\nConclusion\n\n\n\nData Loading and Preprocessing\nFirst, we’ll import the necessary libraries and load our dataset. We’re working with an Excel file containing information about mortgage foreclosure cases.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom category_encoders import TargetEncoder\n\n# Load the data\nfile_path = \"/Users/piru/Desktop/tiempos actuaciones v2 por juzgados v3.xlsx\"\nsheet_name = \"Base3\"\ndf = pd.read_excel(file_path, sheet_name=sheet_name)\nNow that we have our data loaded, we need to clean it. This involves handling missing values, converting date fields, and ensuring all categorical variables are in the correct format.\n# Clean and preprocess data\ndf = df[~df['foreclosure initial date'].apply(lambda x: isinstance(x, time))]\ndf.dropna(axis=1, how='all', inplace=True)\ndf.dropna(axis=0, how='all', inplace=True)\ndf = df.dropna(subset=['Legal Stage'])\n\n# Convert dates and clean text fields\ndf['foreclosure initial date'] = pd.to_datetime(df['foreclosure initial date'])\ndf['resolution date'] = pd.to_datetime(df['resolution date'], errors='coerce')\ndf = df.apply(lambda x: x.str.upper() if x.dtype == 'object' else x)\n\n# Convert boolean columns\nif 'etapa central' in df.columns:\n    df['etapa central'] = df['etapa central'].astype('bool')\nif 'tiene casacion' in df.columns:\n    df['tiene casacion'] = df['tiene casacion'].astype('bool')\nWe also need to ensure that our categorical variables are properly typed. This is important for later encoding steps and for the Random Forest model to handle these variables correctly.\n# Define and apply data types\ndata_types = {\n    'Legal Stage': 'category',\n    'Portfolio Name': 'category',\n    'Legal Proceeding': 'category',\n    'clave': 'category',\n    'resolution type': 'category',\n    'general type': 'category',\n    'zone': 'category',\n    'zone group': 'category',\n    'region': 'category',\n    'Law Firm': 'category',\n    'Judicial Court': 'category',\n}\n\nfor col, dtype in data_types.items():\n    if col in df.columns:\n        df[col] = df[col].astype(dtype)\n\n\nFeature Engineering\nNow that our data is clean, we can start creating new features that might help predict the resolution time. We’ll focus on temporal aspects of the legal proceedings.\nFirst, let’s filter our data to include only foreclosure cases with a positive resolution time:\n# Filter dataframe for FORECLOSURE cases with time to solve &gt; 0.01\ndf = df[(df['general type'] == 'FORECLOSURE') & (df['time to solve'] &gt; 0.01)]\n\n# Remove legal proceedings with less than 5 occurrences\nlegal_proceeding_counts = df['Legal Proceeding'].value_counts()\nvalid_proceedings = legal_proceeding_counts[legal_proceeding_counts &gt;= 5].index\ndf = df[df['Legal Proceeding'].isin(valid_proceedings)]\n\n# Ensure the dataframe is sorted by Legal File Name and Date\ndf = df.sort_values(['Legal File Name', 'Date'])\nNow, let’s create our new features:\n# Calculate new features\ndf['Case_Start'] = df.groupby('Legal File Name')['Date'].transform('min')\ndf['Months_Since_Start'] = (df['Date'] - df['Case_Start']).dt.days / 30.44\ndf['Proceeding_Count'] = df.groupby('Legal File Name').cumcount() + 1\ndf['Months_Since_Last_Proceeding'] = df.groupby('Legal File Name')['Date'].diff().dt.days / 30.44\ndf['Months_Since_Last_Proceeding'] = df['Months_Since_Last_Proceeding'].fillna(1)\ndf['Average_Months_Between_Proceedings'] = df.groupby('Legal File Name')['Months_Since_Last_Proceeding'].transform('mean')\ndf['Distinct_Proceeding_Types'] = df.groupby('Legal File Name')['Legal Proceeding'].transform('nunique')\ndf['Repeated_Proceeding_Count'] = df.groupby(['Legal File Name', 'Legal Proceeding']).cumcount() + 1\ndf['Previous_Proceeding'] = df.groupby('Legal File Name')['Legal Proceeding'].shift(1)\ndf['Previous_Proceeding'] = df['Previous_Proceeding'].fillna(df['Legal Proceeding'])\n\n# Handle missing zone group\ndf['zone group'] = df['zone group'].fillna(df['region'])\ndf['zone group'] = pd.Categorical(df['zone group'])\nThese new features capture various temporal aspects of each case, such as how long the case has been ongoing, how many proceedings have occurred, and the frequency of proceedings.\n\n\nEncoding and Model Preparation\nBefore we can train our model, we need to encode our categorical variables. We’ll use Target Encoding for high-cardinality variables and Label Encoding for others.\n# Target Encoding for high-cardinality categorical variables\nhigh_cardinality_cols = ['Legal Proceeding', 'Judicial Court', 'Law Firm', 'clave', 'Previous_Proceeding']\nte = TargetEncoder()\ndf_encoded = te.fit_transform(df[high_cardinality_cols], df['time to solve'])\n\n# Replace original columns with target encoded versions\nfor col in high_cardinality_cols:\n    df[col] = df_encoded[col]\n\n# Label Encoding for other categorical variables\nle = LabelEncoder()\ncategorical_columns_to_encode = ['Legal Stage', 'Portfolio Name', 'region', 'zone group']\nfor col in categorical_columns_to_encode:\n    df[col] = le.fit_transform(df[col].astype(str))\n    df[col] = pd.Categorical(df[col])\nNow we can define our features and prepare our data for modeling:\n# Define features for modeling\nfeatures = [\n    'Legal Stage', 'Legal Proceeding', 'Portfolio Name', 'zone group', 'region',\n    'Judicial Court', 'Law Firm', 'clave', 'etapa central', 'Months_Since_Start',\n    'Proceeding_Count', 'Months_Since_Last_Proceeding', 'Average_Months_Between_Proceedings',\n    'Distinct_Proceeding_Types', 'Repeated_Proceeding_Count', 'Previous_Proceeding',\n    'time to solve'\n]\n\n# Prepare X and y for modeling\ntarget_col = 'time to solve'\nX = df[features].drop(columns=[target_col])\ny = df[target_col]\n\n\nModel Training and Evaluation\nWe’ll use a Random Forest Regressor for our prediction task. First, let’s split our data into training and testing sets:\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nNext, we need to scale our numerical features:\n# Scale numerical features\nnumerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\nX_train_scaled[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\nX_test_scaled[numerical_columns] = scaler.transform(X_test[numerical_columns])\nNow we can train our Random Forest model:\n# Train Random Forest model\nrf_model = RandomForestRegressor(n_estimators=300, min_samples_split=2, \n                                 min_samples_leaf=1, max_depth=None, \n                                 bootstrap=True, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\nFinally, let’s make predictions and evaluate our model:\n# Make predictions and evaluate model\ny_pred = rf_model.predict(X_test_scaled)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\nprint(f\"R-squared (R2): {r2}\")\nMean Absolute Error (MAE): 4.887682724118453 Root Mean Squared Error (RMSE): 6.985917898599015 R-squared (R2): 0.9382800622825924\n\n\nVisualization\nTo better understand our model’s performance and the importance of our features, we’ll create three visualizations.\nFirst, let’s look at the feature importance:\n# Feature Importance plot\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='importance', y='feature', data=feature_importance.head(20))\nplt.title('Top 20 Feature Importance from Random Forest')\nplt.tight_layout()\nplt.show()\n\n\n\nFeature Importance for Random Forest Model\n\n\nThis plot shows us which features are most influential in our model’s predictions.\nNext, let’s compare our predictions to the actual values:\n# Actual vs Predicted plot\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Actual vs Predicted')\nplt.tight_layout()\nplt.show()\n This plot helps us visualize how well our predictions match the actual values. Points closer to the red line indicate better predictions.\nFinally, let’s look at the residuals of our model:\n# Residual plot\nresiduals = y_test - y_pred\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred, residuals, alpha=0.5)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.tight_layout()\nplt.show()\nThis plot helps us identify any patterns in our model’s errors, which could indicate areas for improvement.\n\n\nConclusion\nThis analysis demonstrates the use of a Random Forest model to predict resolution times for mortgage foreclosure cases. The model shows good performance with an R-squared value of about 0.94, indicating that it explains a large portion of the variance in resolution times.\nKey findings include:\n\nThe importance of temporal features like ‘Months_Since_Start’ and ‘Proceeding_Count’ in predicting resolution times.\nThe model’s ability to capture both linear and non-linear relationships in the data.\nAreas for potential improvement, as seen in the residual plot.\n\nFuture work could involve feature selection, hyperparameter tuning, or exploring other machine learning algorithms to potentially improve the model’s performance."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/My-new-post/index.html",
    "href": "posts/My-new-post/index.html",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "",
    "text": "We developed a new method to predict how long foreclosure cases will take to resolve. This approach is more accurate than our previous method, which will help us make better financial decisions and manage our mortgage portfolio more effectively.\nKey Points:\n\nNew prediction model is about 3 times more accurate than the old one\nIt can explain 94% of the differences in case resolution times, up from 39%\nThis improvement will help us value assets more accurately and make better-informed decisions"
  },
  {
    "objectID": "posts/My-new-post/index.html#summary",
    "href": "posts/My-new-post/index.html#summary",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "",
    "text": "We developed a new method to predict how long foreclosure cases will take to resolve. This approach is more accurate than our previous method, which will help us make better financial decisions and manage our mortgage portfolio more effectively.\nKey Points:\n\nNew prediction model is about 3 times more accurate than the old one\nIt can explain 94% of the differences in case resolution times, up from 39%\nThis improvement will help us value assets more accurately and make better-informed decisions"
  },
  {
    "objectID": "posts/My-new-post/index.html#why-this-matters",
    "href": "posts/My-new-post/index.html#why-this-matters",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "2 Why This Matters",
    "text": "2 Why This Matters\nKnowing how long a foreclosure case will take is crucial for our business. It helps us:\n1. Value our mortgage assets more accurately\n2. Make better decisions about managing our portfolio\n3. Plan our financial strategies more effectively"
  },
  {
    "objectID": "posts/My-new-post/index.html#how-we-made-predictions-before",
    "href": "posts/My-new-post/index.html#how-we-made-predictions-before",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "3 How We Made Predictions Before",
    "text": "3 How We Made Predictions Before\nOur old method was simple but not the most accurate:\n1. We looked at past foreclosure cases\n2. We calculated the average time cases took based on the type of legal proceeding\n3. For new cases, we predicted the resolution time by matching it to similar past cases\n\n3.1 Problems with the Old Method\n\nIt was often wrong by about 17 months on average\nIt couldn’t account for the many factors that make each case unique\nIt often underestimated or overestimated resolution times significantly"
  },
  {
    "objectID": "posts/My-new-post/index.html#our-new-approach",
    "href": "posts/My-new-post/index.html#our-new-approach",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "4 Our New Approach",
    "text": "4 Our New Approach\nWe’ve adopted a more sophisticated method called a “Random Forest” model. Here’s how it works:\n\nIt considers many factors about each case, not just the type of legal proceeding\nIt learns patterns from thousands of past cases\nIt makes predictions based on these learned patterns, considering multiple aspects of each new case\n\n\n4.1 What We Considered in the New Model\n\nHow long the case has been ongoing\nHow many legal proceedings have happened\nHow much time passes between proceedings\nThe specific sequence of legal actions\nDetails about the courts handling the cases"
  },
  {
    "objectID": "posts/My-new-post/index.html#results-how-much-better-is-the-new-model",
    "href": "posts/My-new-post/index.html#results-how-much-better-is-the-new-model",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "5 Results: How Much Better Is the New Model?",
    "text": "5 Results: How Much Better Is the New Model?\nOur new model is significantly more accurate:\n\nAccuracy: On average, predictions are off by only about 5 months, compared to 17 months with the old method\nReliability: The new model can explain 94% of the differences in case resolution times, up from 39%\nConsistency: It handles unusual cases much better, with fewer large errors\n\n\n\n\n\n\n\n\n\n\nRandom Forest Predictions\n\n\n\n\n\n\n\nActual vs Predicted"
  },
  {
    "objectID": "posts/My-new-post/index.html#what-influences-foreclosure-times-the-most",
    "href": "posts/My-new-post/index.html#what-influences-foreclosure-times-the-most",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "6 What Influences Foreclosure Times the Most?",
    "text": "6 What Influences Foreclosure Times the Most?\nOur model found these factors to be most important in predicting resolution times:\n\nHow long the case has been ongoing\nThe number of legal proceedings that have occurred\nThe specific type of legal proceedings involved\nThe region where the case is being handled"
  },
  {
    "objectID": "posts/My-new-post/index.html#what-this-means-for-the-business",
    "href": "posts/My-new-post/index.html#what-this-means-for-the-business",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "7 What This Means for the Business",
    "text": "7 What This Means for the Business\n\nBetter Asset Valuation: We can more accurately estimate the value of our mortgage assets\nImproved Risk Management: We can better anticipate and plan for lengthy foreclosure processes\nSmarter Decision Making: We can make more informed choices about how to handle different cases\nEfficient Resource Allocation: We can better prioritize our efforts and resources across our portfolio"
  },
  {
    "objectID": "posts/My-new-post/index.html#recomendations",
    "href": "posts/My-new-post/index.html#recomendations",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "8 Recomendations",
    "text": "8 Recomendations\n\nExpand Features: Improve model accuracy by adding features like Loan-to-Value (LTV) ratios, to help predict whether a foreclosure will be resolved by a third party or creditor, along with economic indicators or more detailed geographic data..\n\n\n\nExplore Advanced Techniques: Test advanced machine learning techniques for further improvement.\nIntegrate with Financial Tools: Incorporate the model’s predictions into financial systems to enhance decision-making and structure DPO agreements."
  },
  {
    "objectID": "posts/My-new-post/index.html#conclusion",
    "href": "posts/My-new-post/index.html#conclusion",
    "title": "Improving Foreclosure Timeline Predictions(Simple Explanation)",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nOur new foreclosure prediction model represents a significant improvement in our ability to manage mortgage portfolio. By providing more accurate and reliable predictions, it will help us make better-informed decisions, manage risks more effectively, and ultimately improve our financial performance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Foreclosure Timeline Predictions(Simple Explanation)\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nPiru Puiggari\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Predictive Model Accuracy in Foreclosure Cases\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nPiru Puiggari\n\n\n\n\n\n\nNo matching items"
  }
]